{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"latent space analysis.ipynb","private_outputs":true,"provenance":[{"file_id":"1A5BHiqwrlNb2vY3-Sn4AtyDJ42np2Rbt","timestamp":1607965120215},{"file_id":"1WAGUPlR-Y2swsp2Tg-b4trc0FvmkGTbO","timestamp":1607919258929}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PPbRsg4ilfIC"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LogykzScDkOD"},"source":["from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","import os\n","import sys\n","import shutil\n","import numpy as np\n","from itertools import cycle,islice\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","from torch.nn.parameter import Parameter\n","from torch.utils.data import Dataset, IterableDataset, DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from itertools import combinations\n","from tqdm import tqdm\n","import json\n","# from paths import *\n","\n","project_dir = \"/content/drive/MyDrive/ift6269/project\"\n","os.chdir(project_dir)\n","data_dir = os.path.join(project_dir, \"data\")\n","rec_data_dir = os.path.join(data_dir, \"recordings\")\n","mnist_data_dir = os.path.join(data_dir, \"mnist\")\n","speech_data_dir = os.path.join(data_dir, \"speech\")\n","lookup_embd_dir = os.path.join(data_dir, \"lookup_embd\")\n","OUT_DIR = os.path.join(project_dir, \"modelnew\")\n","\n","from entity import person\n","from retabulate import tabulate,init_history,mean_history\n","import matplotlib.pyplot as plt\n","import pickle as pkl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QncLQEJNGG6E"},"source":["N_LATENTS=512\n","CUDA=torch.cuda.is_available()\n","EPOCHS=500 \n","# OUT_DIR=\"/content/drive/MyDrive/project/modelnew/\"\n","lambda_image=1\n","lambda_speech=100\n","learning_rate= 1e-3\n","annealing_epochs=200\n","log_interval=100\n","batch_size= 128\n","N_mini_batches=48000//batch_size\n","N_mini_batches_val=188\n","N_mini_batches_test=317"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3gcdLKcGwCQ"},"source":["p = person(\"Farsi\", 'Kannada',dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWQQiL6HD9-I"},"source":["class MyIterableDataset(IterableDataset):\n","    def __init__(self,p, mode):\n","        self.data=p\n","        self.mode=mode\n","    def parse_data(self):\n","        if self.mode==\"train\":\n","            for sample in self.data._sample(self.data.mnist_1_X_train, \n","                                            self.data.mnist_2_X_train, \n","                                            self.data.speech_X_train, \n","                                            self.data.mnist_1_y_train, \n","                                            self.data.mnist_2_y_train, \n","                                            self.data.speech_y_train):\n","                yield sample\n","        elif self.mode==\"val\":\n","            for sample in self.data._sample(self.data.mnist_1_X_valid, \n","                                            self.data.mnist_2_X_valid, \n","                                            self.data.speech_X_valid, \n","                                            self.data.mnist_1_y_valid, \n","                                            self.data.mnist_2_y_valid, \n","                                            self.data.speech_y_valid):\n","                yield sample\n","        else:\n","            for sample in self.data._sample(self.data.mnist_1_X_test, \n","                                            self.data.mnist_2_X_test, \n","                                            self.data.speech_X_test, \n","                                            self.data.mnist_1_y_test, \n","                                            self.data.mnist_2_y_test, \n","                                            self.data.speech_y_test):\n","                yield sample\n","    # def get_stream(self):\n","    #     return cycle(self.parse_data())\n","    def __iter__(self):\n","        return self.parse_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyeBmL4_ytn3"},"source":["iterable_dataset=MyIterableDataset(p, mode=\"train\")\n","train_loader=DataLoader(iterable_dataset,batch_size=batch_size)\n","iterable_dataset_val=MyIterableDataset(p, mode=\"val\")\n","val_loader=DataLoader(iterable_dataset_val,1)\n","iterable_dataset_test=MyIterableDataset(p, mode=\"test\")\n","test_loader=DataLoader(iterable_dataset_test,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jrzm-wQoASQf"},"source":["\n","for batch_idx,((mnist1, mnist2, speech), label_y) in enumerate(test_loader):\n","    # print(mnist2)\n","    break\n","# print(batch_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZk8MhFp1op2"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"2vUFvBE9zcR9"},"source":["class MVAE(nn.Module):\n","    \"\"\"Multimodal Variational Autoencoder.\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(MVAE, self).__init__()\n","        self.image1_encoder = ImageEncoder(n_latents)\n","        # self.image1_encoder=self.image1_encoder.apply(self.weights_init)\n","        self.image1_decoder = ImageDecoder(n_latents)\n","        self.image2_encoder = ImageEncoder(n_latents)\n","        # self.image2_encoder=self.image2_encoder.apply(self.weights_init)\n","        self.image2_decoder = ImageDecoder(n_latents)\n","        self.speech_encoder  = SpeechEncoder(n_latents)\n","        # self.speech_encoder=self.speech_encoder.apply(self.weights_init)\n","        self.speech_decoder  = SpeechDecoder(n_latents)\n","        self.label_decoder = LabelDecoder(n_latents)\n","\n","        self.experts       = ProductOfExperts()\n","        self.n_latents     = n_latents\n","    def weights_init(self,m):\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.zeros_(m.weight)\n","            torch.nn.init.zeros_(m.bias)\n","    \n","    def reparametrize(self, mu, logvar):\n","        if self.training:\n","            std = logvar.mul(0.5).exp_()\n","            eps = Variable(std.data.new(std.size()).normal_())\n","            return eps.mul(std).add_(mu)\n","        else:\n","          return mu\n","\n","    def forward(self, image1=None,image2=None, speech=None):\n","        mu, logvar = self.infer(image1,image2, speech)\n","        # reparametrization trick to sample\n","        z          = self.reparametrize(mu, logvar)\n","        # reconstruct inputs based on that gaussian\n","        img1_recon  = self.image1_decoder(z)\n","        img2_recon  = self.image2_decoder(z)\n","        sp_recon  = self.speech_decoder(z)\n","        label_recon=self.label_decoder(z)\n","        return img1_recon,img2_recon, sp_recon,label_recon, mu, logvar\n","\n","    def infer(self, image1=None,image2=None, speech=None):\n","\n","        batch_size = image1.size(0) if image1 is not None else image2.size(0) if image2 is not None else speech.size(0)\n","        use_cuda   = next(self.parameters()).is_cuda  # check if CUDA\n","        # initialize the universal prior expert\n","        mu, logvar = prior_expert((1, batch_size, self.n_latents), \n","                                  use_cuda=use_cuda)\n","        if image1 is not None:\n","            img1_mu, img1_logvar = self.image1_encoder(image1)\n","            mu     = torch.cat((mu, img1_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, img1_logvar.unsqueeze(0)), dim=0)\n","        if image2 is not None:\n","            img2_mu, img2_logvar = self.image2_encoder(image2)\n","            mu     = torch.cat((mu, img2_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, img2_logvar.unsqueeze(0)), dim=0)\n","\n","        if speech is not None:\n","            sp_mu, sp_logvar = self.speech_encoder(speech)\n","            mu     = torch.cat((mu, sp_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, sp_logvar.unsqueeze(0)), dim=0)\n","\n","        # product of experts to combine gaussians\n","        mu, logvar = self.experts(mu, logvar)\n","        return mu, logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q002ywtZzoNA"},"source":["class ImageEncoder(nn.Module):\n","    \"\"\"Parametrizes q(z|x).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(ImageEncoder, self).__init__()\n","        self.fc1   = nn.Linear(784, 2048)\n","        self.fc1_2   = nn.Linear(2048, 1024)\n","        self.fc2   = nn.Linear(1024, 512)\n","        self.fc31  = nn.Linear(512, n_latents)\n","        self.fc32  = nn.Linear(512, n_latents).apply(self.weights_init)\n","        self.swish = Swish()\n","    def weights_init(self,m):\n","        torch.nn.init.zeros_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","    def forward(self, x):\n","        xx=x.view(-1, 784)\n","        h= self.fc1(xx.float())\n","        h = self.swish(h)\n","        h = self.swish(self.fc1_2(h))\n","        h = self.swish(self.fc2(h))\n","        return self.fc31(h), self.fc32(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VX0LilO0Pke"},"source":["class ImageDecoder(nn.Module):\n","    \"\"\"Parametrizes p(x|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(ImageDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc3_4   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 784)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        h = self.swish(self.fc3_4(h))\n","        return self.fc4(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLFbn92N0Qp-"},"source":["class SpeechEncoder(nn.Module):\n","    \"\"\"Parametrizes q(z|y).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(SpeechEncoder, self).__init__()\n","        self.fc1   = nn.Linear(13, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc31  = nn.Linear(512, n_latents)\n","        self.fc32  = nn.Linear(512, n_latents).apply(self.weights_init)\n","        self.swish = Swish()\n","    def weights_init(self,m):\n","        torch.nn.init.zeros_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","    def forward(self, x):\n","        h = self.swish(self.fc1(x.float()))\n","        h = self.swish(self.fc2(h))\n","        return self.fc31(h), self.fc32(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5E-nQZV1GkL"},"source":["#TODO\n","class SpeechDecoder(nn.Module):\n","    \"\"\"Parametrizes p(y|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(SpeechDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 13)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        return self.fc4(h)  # NOTE: no softmax here. See train.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBoiIxf5zr6n"},"source":["class LabelDecoder(nn.Module):\n","    \"\"\"Parametrizes p(y|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(LabelDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 10)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        return self.fc4(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYN06wza1M4E"},"source":["class ProductOfExperts(nn.Module):\n","    \"\"\"Return parameters for product of independent experts.\n","    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n","    @param mu: M x D for M experts\n","    @param logvar: M x D for M experts\n","    \"\"\"\n","    def forward(self, mu, logvar, eps=1e-8):\n","        var       = torch.exp(logvar) + eps\n","        # precision of i-th Gaussian expert at point x\n","        T         = 1. / (var + eps)\n","        pd_mu     = torch.sum(mu * T, dim=0) / torch.sum(T, dim=0)\n","        pd_var    = 1. / torch.sum(T, dim=0)\n","        pd_logvar = torch.log(pd_var + eps)\n","        return pd_mu, pd_logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZ9tZKMA1S3F"},"source":["class Swish(nn.Module):\n","    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n","    def forward(self, x):\n","        # return x * F.sigmoid(x)\n","        return x *( torch.tanh(F.softplus(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98mn2dwpGmJ-"},"source":["class Mish(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n","        return x *( torch.tanh(F.softplus(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyPQoHPJ1bl0"},"source":["def prior_expert(size, use_cuda=False):\n","    \"\"\"Universal prior expert. Here we use a spherical\n","    Gaussian: N(0, 1).\n","    @param size: integer\n","                 dimensionality of Gaussian\n","    @param use_cuda: boolean [default: False]\n","                     cast CUDA on variables\n","    \"\"\"\n","    mu     = Variable(torch.zeros(size))\n","    logvar = Variable(torch.zeros(size))\n","    if use_cuda:\n","        mu, logvar = mu.cuda(), logvar.cuda()\n","    return mu, logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMlvcD4271Yg"},"source":["def save_checkpoint(state, is_best, folder='./', filename='checkpoint.pth.tar'):\n","    if not os.path.isdir(folder):\n","        os.mkdir(folder)\n","    torch.save(state, os.path.join(folder, filename))\n","    if is_best:\n","        shutil.copyfile(os.path.join(folder, filename),\n","                        os.path.join(folder, 'model_best.pth.tar'))\n","\n","\n","def load_checkpoint(file_path, use_cuda=False):\n","    checkpoint = torch.load(file_path) if use_cuda else \\\n","        torch.load(file_path, map_location=lambda storage, location: storage)\n","    model = MVAE(checkpoint['n_latents'])\n","    model.load_state_dict(checkpoint['state_dict'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u236A9ofOEsl"},"source":["# Load Model"]},{"cell_type":"code","metadata":{"id":"6fRpCIJXlmI6"},"source":["model=load_checkpoint(OUT_DIR+'/checkpoint.pth.tar',use_cuda=True)\r\n","device = torch.device(\"cuda\")\r\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DjRg-eNLYeD"},"source":["def get_mu_logvar(digit=7):\r\n","  model.eval()\r\n","  val_loss = 0\r\n","  count=0\r\n","  # for simplicitly, here i'm only going to track the joint loss. \r\n","  # pbar = tqdm(total=len(test_loader))\r\n","  for batch_idx, ((image1,image2,speech),y) in enumerate(val_loader):\r\n","      imorig1=image1\r\n","      imorig2=image2\r\n","      label=np.argmax(y).cpu().detach().numpy()\r\n","      if label == digit: \r\n","        # print(y) \r\n","        break\r\n","  if CUDA:\r\n","      image1     = image1.cuda().float()\r\n","      image2     = image2.cuda().float()\r\n","      speech     = speech.cuda().float()\r\n","      y     = y.cuda().float()\r\n","  image1         = Variable(image1)\r\n","  image2         = Variable(image2)\r\n","  speech         = Variable(speech)\r\n","  y         = Variable(y)\r\n","  batch_size = image1.size(0)\r\n","\r\n","  recon_image1, recon_image2, recon_speech, recon_label, mu, logvar = model(image1=image1, image2=image2, speech=speech)\r\n","\r\n","  return mu, logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQMrd_wq5-gu"},"source":["def show_image1(model, Tens):\r\n","  image1_recons=model.image1_decoder(Tens)\r\n","  image1_recons=image1_recons.view(-1,28,28)\r\n","  w=10\r\n","  h=10\r\n","  fig=plt.figure(figsize=(10, 10))\r\n","  columns = 10\r\n","  rows = 10\r\n","  for i in range(0, columns*rows):\r\n","      img = image1_recons[i,:,:].cpu().detach().numpy()\r\n","      fig.add_subplot(rows, columns, i+1)\r\n","      plt.axis('off')\r\n","      # fig.set_aspect('equal')\r\n","      plt.subplots_adjust(wspace=0, hspace=0)\r\n","      plt.imshow(img,interpolation='nearest')\r\n","  # plt.show()\r\n","  # plt.savefig(OUT_DIR+\"/Fa_7(5,3,7,0).png\",bbox_inches='tight', pad_inches = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkBkO3bkQIjh"},"source":["def show_image2(model, Tens):\n","  image2_recons=model.image2_decoder(Tens)\n","  image2_recons=image2_recons.view(-1,28,28)\n","  fig=plt.figure(figsize=(10, 10))\n","  columns = 10\n","  rows = 10\n","  for i in range(0, columns*rows):\n","      img = image2_recons[i,:,:].cpu().detach().numpy()\n","      fig.add_subplot(rows, columns, i+1)\n","      plt.axis('off')\n","      # fig.set_aspect('equal')\n","      plt.subplots_adjust(wspace=0, hspace=0)\n","      plt.imshow(img,interpolation='nearest')\n","  # plt.show()\n","  # plt.savefig(OUT_DIR+\"/Ka_7(5,3,7,0).png\",bbox_inches='tight', pad_inches = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oviV3lWh1C6C"},"source":["from copy import deepcopy\r\n","def discover_disentanglement(z, idx, change, num=100):\r\n","  # z= model.reparametrize(mu,logvar).clone().detach()\r\n","  Z = torch.randn(num,512).to(device)\r\n","  sub = [-change*i for i in range(1,num//2+1)][::-1]\r\n","  add = [change*i for i in range(1,num//2+1)]\r\n","  # print(sub+add)\r\n","  for i, ch in enumerate(sub+add):\r\n","    Z[i] = deepcopy(z)\r\n","    Z[i][idx] += ch \r\n","  return Z\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcoo89YDsGqJ"},"source":["denorm  = lambda x: x*0.5+0.5\n","def plot_line(img_tensor,save=False,name='default'):\n","    _,axs = plt.subplots(1,11,figsize=(10,5))\n","    img_tensor = denorm(img_tensor.cpu())\n","    for i,ax in enumerate(axs.flatten()):\n","        img = img_tensor[i].detach().numpy()\n","        ax.imshow(img)\n","        ax.axis('off')\n","    if save: plt.savefig(name)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nh_k0Jr3uZOX"},"source":["def get_z(digit, model):\n","  mu, logvar = get_mu_logvar(digit=digit)\n","  return model.reparametrize(mu,logvar)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUO-juBrx5RM"},"source":["# Disentanglement"]},{"cell_type":"code","metadata":{"id":"GTltx_9ErCGd"},"source":["z = get_z(digit=7, model=model)\r\n","tens=discover_disentanglement(z.clone().detach(), 201,30)\r\n","show_image1(model, tens)\r\n","show_image2(model, tens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ezXKoEGx75Q"},"source":["# Transition / Arithematics in latent space"]},{"cell_type":"code","metadata":{"id":"mYfUL639s0KZ"},"source":["z0 = get_z(digit=5, model=model)\n","z1 = get_z(digit=0, model=model)\n","alpha = torch.arange(0,1.1,0.1).to(device).view(-1,1)\n","z_dash_a = alpha*z1 + (1-alpha)*z0\n","image_tensor_1 = model.image1_decoder(z_dash_a).view(-1,28,28)\n","image_tensor_2 = model.image2_decoder(z_dash_a).view(-1,28,28)\n","plot_line(image_tensor_1,False,'line1')\n","plot_line(image_tensor_2,False,'line2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptT4ow4UxoHf"},"source":[""],"execution_count":null,"outputs":[]}]}