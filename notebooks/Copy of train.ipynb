{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of train.ipynb","private_outputs":true,"provenance":[{"file_id":"1WAGUPlR-Y2swsp2Tg-b4trc0FvmkGTbO","timestamp":1607919258929}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PPbRsg4ilfIC"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LogykzScDkOD"},"source":["from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","import os\n","import sys\n","import shutil\n","import numpy as np\n","from itertools import cycle,islice\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","from torch.nn.parameter import Parameter\n","from torch.utils.data import Dataset, IterableDataset, DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from itertools import combinations\n","from tqdm import tqdm\n","import json\n","os.chdir(\"/content/drive/MyDrive/project\")\n","from entity import person\n","from paths import *\n","from retabulate import tabulate,init_history,mean_history\n","import matplotlib.pyplot as plt\n","import pickle as pkl"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hapUrtMQfFyM"},"source":["os.makedirs(\"/content/drive/MyDrive/project/modelnew/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QncLQEJNGG6E"},"source":["N_LATENTS=512\n","CUDA=torch.cuda.is_available()\n","EPOCHS=500 \n","OUT_DIR=\"/content/drive/MyDrive/project/modelnew/\"\n","lambda_image=1\n","lambda_speech=100\n","learning_rate= 1e-3\n","annealing_epochs=200\n","log_interval=100\n","batch_size= 128\n","N_mini_batches=48000//batch_size\n","N_mini_batches_val=188\n","N_mini_batches_test=317"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3gcdLKcGwCQ"},"source":["p = person(\"Farsi\", 'Kannada',dtype=np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rWQQiL6HD9-I"},"source":["class MyIterableDataset(IterableDataset):\n","    def __init__(self,p, mode):\n","        self.data=p\n","        self.mode=mode\n","    def parse_data(self):\n","        if self.mode==\"train\":\n","            for sample in self.data._sample(self.data.mnist_1_X_train, \n","                                            self.data.mnist_2_X_train, \n","                                            self.data.speech_X_train, \n","                                            self.data.mnist_1_y_train, \n","                                            self.data.mnist_2_y_train, \n","                                            self.data.speech_y_train):\n","                yield sample\n","        elif self.mode==\"val\":\n","            for sample in self.data._sample(self.data.mnist_1_X_valid, \n","                                            self.data.mnist_2_X_valid, \n","                                            self.data.speech_X_valid, \n","                                            self.data.mnist_1_y_valid, \n","                                            self.data.mnist_2_y_valid, \n","                                            self.data.speech_y_valid):\n","                yield sample\n","        else:\n","            for sample in self.data._sample(self.data.mnist_1_X_test, \n","                                            self.data.mnist_2_X_test, \n","                                            self.data.speech_X_test, \n","                                            self.data.mnist_1_y_test, \n","                                            self.data.mnist_2_y_test, \n","                                            self.data.speech_y_test):\n","                yield sample\n","    # def get_stream(self):\n","    #     return cycle(self.parse_data())\n","    def __iter__(self):\n","        return self.parse_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyeBmL4_ytn3"},"source":["iterable_dataset=MyIterableDataset(p, mode=\"train\")\n","train_loader=DataLoader(iterable_dataset,batch_size=batch_size)\n","iterable_dataset_val=MyIterableDataset(p, mode=\"val\")\n","val_loader=DataLoader(iterable_dataset_val,1)\n","iterable_dataset_test=MyIterableDataset(p, mode=\"test\")\n","test_loader=DataLoader(iterable_dataset_test,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jrzm-wQoASQf"},"source":["\n","for batch_idx,((mnist1, mnist2, speech), label_y) in enumerate(test_loader):\n","    # print(mnist2)\n","    break\n","# print(batch_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZk8MhFp1op2"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"2vUFvBE9zcR9"},"source":["class MVAE(nn.Module):\n","    \"\"\"Multimodal Variational Autoencoder.\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(MVAE, self).__init__()\n","        self.image1_encoder = ImageEncoder(n_latents)\n","        # self.image1_encoder=self.image1_encoder.apply(self.weights_init)\n","        self.image1_decoder = ImageDecoder(n_latents)\n","        self.image2_encoder = ImageEncoder(n_latents)\n","        # self.image2_encoder=self.image2_encoder.apply(self.weights_init)\n","        self.image2_decoder = ImageDecoder(n_latents)\n","        self.speech_encoder  = SpeechEncoder(n_latents)\n","        # self.speech_encoder=self.speech_encoder.apply(self.weights_init)\n","        self.speech_decoder  = SpeechDecoder(n_latents)\n","        self.label_decoder = LabelDecoder(n_latents)\n","\n","        self.experts       = ProductOfExperts()\n","        self.n_latents     = n_latents\n","    def weights_init(self,m):\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.zeros_(m.weight)\n","            torch.nn.init.zeros_(m.bias)\n","    \n","    def reparametrize(self, mu, logvar):\n","        if self.training:\n","            std = logvar.mul(0.5).exp_()\n","            eps = Variable(std.data.new(std.size()).normal_())\n","            return eps.mul(std).add_(mu)\n","        else:\n","          return mu\n","\n","    def forward(self, image1=None,image2=None, speech=None):\n","        mu, logvar = self.infer(image1,image2, speech)\n","        # reparametrization trick to sample\n","        z          = self.reparametrize(mu, logvar)\n","        # reconstruct inputs based on that gaussian\n","        img1_recon  = self.image1_decoder(z)\n","        img2_recon  = self.image2_decoder(z)\n","        sp_recon  = self.speech_decoder(z)\n","        label_recon=self.label_decoder(z)\n","        return img1_recon,img2_recon, sp_recon,label_recon, mu, logvar\n","\n","    def infer(self, image1=None,image2=None, speech=None):\n","\n","        batch_size = image1.size(0) if image1 is not None else image2.size(0) if image2 is not None else speech.size(0)\n","        use_cuda   = next(self.parameters()).is_cuda  # check if CUDA\n","        # initialize the universal prior expert\n","        mu, logvar = prior_expert((1, batch_size, self.n_latents), \n","                                  use_cuda=use_cuda)\n","        if image1 is not None:\n","            img1_mu, img1_logvar = self.image1_encoder(image1)\n","            mu     = torch.cat((mu, img1_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, img1_logvar.unsqueeze(0)), dim=0)\n","        if image2 is not None:\n","            img2_mu, img2_logvar = self.image2_encoder(image2)\n","            mu     = torch.cat((mu, img2_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, img2_logvar.unsqueeze(0)), dim=0)\n","\n","        if speech is not None:\n","            sp_mu, sp_logvar = self.speech_encoder(speech)\n","            mu     = torch.cat((mu, sp_mu.unsqueeze(0)), dim=0)\n","            logvar = torch.cat((logvar, sp_logvar.unsqueeze(0)), dim=0)\n","\n","        # product of experts to combine gaussians\n","        mu, logvar = self.experts(mu, logvar)\n","        return mu, logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iJKeC-YkVSJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q002ywtZzoNA"},"source":["class ImageEncoder(nn.Module):\n","    \"\"\"Parametrizes q(z|x).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(ImageEncoder, self).__init__()\n","        self.fc1   = nn.Linear(784, 2048)\n","        self.fc1_2   = nn.Linear(2048, 1024)\n","        self.fc2   = nn.Linear(1024, 512)\n","        self.fc31  = nn.Linear(512, n_latents)\n","        self.fc32  = nn.Linear(512, n_latents).apply(self.weights_init)\n","        self.swish = Swish()\n","    def weights_init(self,m):\n","        torch.nn.init.zeros_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","    def forward(self, x):\n","        xx=x.view(-1, 784)\n","        h= self.fc1(xx.float())\n","        h = self.swish(h)\n","        h = self.swish(self.fc1_2(h))\n","        h = self.swish(self.fc2(h))\n","        return self.fc31(h), self.fc32(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VX0LilO0Pke"},"source":["class ImageDecoder(nn.Module):\n","    \"\"\"Parametrizes p(x|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(ImageDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc3_4   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 784)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        h = self.swish(self.fc3_4(h))\n","        return self.fc4(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLFbn92N0Qp-"},"source":["class SpeechEncoder(nn.Module):\n","    \"\"\"Parametrizes q(z|y).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(SpeechEncoder, self).__init__()\n","        self.fc1   = nn.Linear(13, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc31  = nn.Linear(512, n_latents)\n","        self.fc32  = nn.Linear(512, n_latents).apply(self.weights_init)\n","        self.swish = Swish()\n","    def weights_init(self,m):\n","        torch.nn.init.zeros_(m.weight)\n","        torch.nn.init.zeros_(m.bias)\n","    def forward(self, x):\n","        h = self.swish(self.fc1(x.float()))\n","        h = self.swish(self.fc2(h))\n","        return self.fc31(h), self.fc32(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5E-nQZV1GkL"},"source":["#TODO\n","class SpeechDecoder(nn.Module):\n","    \"\"\"Parametrizes p(y|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(SpeechDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 13)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        return self.fc4(h)  # NOTE: no softmax here. See train.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBoiIxf5zr6n"},"source":["class LabelDecoder(nn.Module):\n","    \"\"\"Parametrizes p(y|z).\n","    @param n_latents: integer\n","                      number of latent dimensions\n","    \"\"\"\n","    def __init__(self, n_latents):\n","        super(LabelDecoder, self).__init__()\n","        self.fc1   = nn.Linear(n_latents, 512)\n","        self.fc2   = nn.Linear(512, 512)\n","        self.fc3   = nn.Linear(512, 512)\n","        self.fc4   = nn.Linear(512, 10)\n","        self.swish = Swish()\n","\n","    def forward(self, z):\n","        h = self.swish(self.fc1(z))\n","        h = self.swish(self.fc2(h))\n","        h = self.swish(self.fc3(h))\n","        return self.fc4(h)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYN06wza1M4E"},"source":["class ProductOfExperts(nn.Module):\n","    \"\"\"Return parameters for product of independent experts.\n","    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n","    @param mu: M x D for M experts\n","    @param logvar: M x D for M experts\n","    \"\"\"\n","    def forward(self, mu, logvar, eps=1e-8):\n","        var       = torch.exp(logvar) + eps\n","        # precision of i-th Gaussian expert at point x\n","        T         = 1. / (var + eps)\n","        pd_mu     = torch.sum(mu * T, dim=0) / torch.sum(T, dim=0)\n","        pd_var    = 1. / torch.sum(T, dim=0)\n","        pd_logvar = torch.log(pd_var + eps)\n","        return pd_mu, pd_logvar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZ9tZKMA1S3F"},"source":["class Swish(nn.Module):\n","    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n","    def forward(self, x):\n","        # return x * F.sigmoid(x)\n","        return x *( torch.tanh(F.softplus(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98mn2dwpGmJ-"},"source":["class Mish(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n","        return x *( torch.tanh(F.softplus(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyPQoHPJ1bl0"},"source":["def prior_expert(size, use_cuda=False):\n","    \"\"\"Universal prior expert. Here we use a spherical\n","    Gaussian: N(0, 1).\n","    @param size: integer\n","                 dimensionality of Gaussian\n","    @param use_cuda: boolean [default: False]\n","                     cast CUDA on variables\n","    \"\"\"\n","    mu     = Variable(torch.zeros(size))\n","    logvar = Variable(torch.zeros(size))\n","    if use_cuda:\n","        mu, logvar = mu.cuda(), logvar.cuda()\n","    return mu, logvar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zddHmTNe13g8"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"ybN3FuMF155B"},"source":["def elbo_loss(recon_image1=None, image1=None,recon_image2=None, image2=None, recon_speech=None, speech=None,recon_label=None,y=None, mu=None, logvar=None,\n","              lambda_image=1.0, lambda_speech=1.0, annealing_factor=1):\n","    \"\"\"Compute the ELBO for an arbitrary number of data modalities.\n","    @param recon: list of torch.Tensors/Variables\n","                  Contains one for each modality.\n","    @param data: list of torch.Tensors/Variables\n","                 Size much agree with recon.\n","    @param mu: Torch.Tensor\n","               Mean of the variational distribution.\n","    @param logvar: Torch.Tensor\n","                   Log variance for variational distribution.\n","    @param lambda_image: float [default: 1.0]\n","                         weight for image BCE\n","    @param lambda_attr: float [default: 1.0]\n","                        weight for attribute BCE\n","    @param annealing_factor: float [default: 1]\n","                             Beta - how much to weight the KL regularizer.\n","    \"\"\"\n","    # assert len(recon) == len(data), \"must supply ground truth for every modality.\"\n","    # n_modalities = len(recon)\n","    batch_size   = mu.size(0)\n","\n","\n","    image1_bce,image2_bce,speech_mse,label_ce  = 0,0,0,0  # reconstruction cost\n","    if recon_image1 is not None and image1 is not None:\n","        # image1_bce = torch.mean(torch.sum(binary_cross_entropy_with_logits(\n","        #     recon_image1.view(-1, 1 * 28 * 28), \n","        #     image1.view(-1, 1 * 28 * 28)), dim=1))\n","        image1_bce= torch.mean(torch.sum(nn.BCEWithLogitsLoss(reduction='none')(\n","            recon_image1.view(-1, 1 * 28 * 28), \n","            image1.view(-1, 1 * 28 * 28)),dim=1),dim=0)\n","        # image1_bce= torch.mean(torch.sum(nn.MSELoss(reduction='none')(\n","        #     recon_image1.view(-1, 1 * 28 * 28), \n","        #     image1.view(-1, 1 * 28 * 28)),dim=1),dim=0)\n","    # print(\"IMAGE1 RECONSTRUCTION\",image1_bce)\n","    if recon_image2 is not None and image2 is not None:\n","        # image2_bce = torch.mean(torch.sum(binary_cross_entropy_with_logits(\n","        #     recon_image2.view(-1, 1 * 28 * 28), \n","        #     image2.view(-1, 1 * 28 * 28)), dim=1))\n","        image2_bce= torch.mean(torch.sum(nn.BCEWithLogitsLoss(reduction='none')(\n","            recon_image2.view(-1, 1 * 28 * 28), \n","            image2.view(-1, 1 * 28 * 28)),dim=1),dim=0)\n","        # image2_bce= torch.mean(torch.sum(nn.MSELoss(reduction='none')(\n","        #     recon_image2.view(-1, 1 * 28 * 28), \n","        #     image2.view(-1, 1 * 28 * 28)),dim=1),dim=0)\n","    # print(\"IMAGE2 RECONSTRUCTION\",image2_bce)    \n","    if recon_speech is not None and speech is not None:  # this is for an attribute\n","        # print(recon_speech,speech, type(recon_speech),type(speech))\n","        loss= nn.MSELoss(reduction='none')\n","        speech_mse = torch.mean(torch.sum(loss(recon_speech, speech),dim=1),dim=0)\n","    # print(\"SPEECH RECONSTRUCTION\",speech_mse)\n","    if recon_label is not None and y is not None:\n","        label_ce=torch.mean(nn.CrossEntropyLoss(reduction='none')(recon_label,torch.argmax(y,dim=1)),dim=0)\n","    # print(\"CLASSIFICATION\",label_ce)\n","    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1),dim=0)\n","    # print(\"KL DIVERGENCE\",KLD)\n","    # KLD=0\n","    ELBO = lambda_image * image1_bce + lambda_image * image2_bce + lambda_speech * speech_mse+label_ce + annealing_factor * KLD\n","    return ELBO,image1_bce,image2_bce,speech_mse,label_ce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKzdBgDk7WXf"},"source":["def binary_cross_entropy_with_logits(input, target):\n","    \"\"\"Sigmoid Activation + Binary Cross Entropy\n","    @param input: torch.Tensor (size N)\n","    @param target: torch.Tensor (size N)\n","    @return loss: torch.Tensor (size N)\n","    \"\"\"\n","    if not (target.size() == input.size()):\n","        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(\n","            target.size(), input.size()))\n","\n","    return (torch.clamp(input, 0) - input * target \n","            + torch.log(1 + torch.exp(-torch.abs(input))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdICflg5VGpF"},"source":["def cross_entropy(input, target, eps=1e-6):\n","    \"\"\"k-Class Cross Entropy (Log Softmax + Log Loss)\n","    \n","    @param input: torch.Tensor (size N x K)\n","    @param target: torch.Tensor (size N x K)\n","    @param eps: error to add (default: 1e-6)\n","    @return loss: torch.Tensor (size N)\n","    \"\"\"\n","    if not (target.size(0) == input.size(0)):\n","        raise ValueError(\n","            \"Target size ({}) must be the same as input size ({})\".format(\n","                target.size(0), input.size(0)))\n","\n","    log_input = F.log_softmax(input + eps, dim=1)\n","    y_onehot = Variable(log_input.data.new(log_input.size()).zero_())\n","    y_onehot = y_onehot.int().scatter_(1, target.unsqueeze(1), 1)\n","    loss = y_onehot * log_input\n","    return -loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dcQ86xTDv-7"},"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMlvcD4271Yg"},"source":["def save_checkpoint(state, is_best, folder='./', filename='checkpoint.pth.tar'):\n","    if not os.path.isdir(folder):\n","        os.mkdir(folder)\n","    torch.save(state, os.path.join(folder, filename))\n","    if is_best:\n","        shutil.copyfile(os.path.join(folder, filename),\n","                        os.path.join(folder, 'model_best.pth.tar'))\n","\n","\n","def load_checkpoint(file_path, use_cuda=False):\n","    checkpoint = torch.load(file_path) if use_cuda else \\\n","        torch.load(file_path, map_location=lambda storage, location: storage)\n","    model = MVAE(checkpoint['n_latents'])\n","    model.load_state_dict(checkpoint['state_dict'])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hj7Kev59_aO5"},"source":["\n","# if not os.path.isdir('./trained_models'):\n","#     os.makedirs('./trained_models')\n","\n","# crop the input image to 64 x 64\n","preprocess_data = transforms.Compose([transforms.Resize(64),\n","                                        transforms.CenterCrop(64),\n","                                        transforms.ToTensor()])\n","\n","model     = MVAE(N_LATENTS)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate,amsgrad=True)\n","# optimizer=optim.RMSprop(model.parameters(),lr=learning_rate)\n","if CUDA:\n","    model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEUUXn_5Erob"},"source":["def train(epoch,History):\n","    model.train()\n","    train_loss_meter = AverageMeter()\n","    history=init_history()\n","    for batch_idx, ((image1,image2,speech),y) in enumerate(train_loader):\n","        if epoch < annealing_epochs:\n","            # compute the KL annealing factor for the current mini-batch in the current epoch\n","            annealing_factor = (float(batch_idx + (epoch - 1) * N_mini_batches + 1) /\n","                                float(annealing_epochs * N_mini_batches))\n","        else:\n","            # by default the KL annealing factor is unity\n","            annealing_factor = 1.0\n","\n","        if CUDA:\n","            image1     = image1.cuda().float()\n","            image2     = image2.cuda().float()\n","            speech     = speech.cuda().float()\n","            y     = y.cuda()\n","        image1         = Variable(image1)\n","        image2         = Variable(image2)\n","        speech         = Variable(speech)\n","        y         = Variable(y)\n","          # convert tensor to list\n","        batch_size    = len(image1)\n","\n","        # refresh the optimizer\n","        optimizer.zero_grad()\n","\n","        # train_loss    = 0  # accumulate train loss here so we don't store a lot of things.\n","        history,train_loss= tabulate(model,history,image1,image2,speech,y,batch_idx,annealing_factor,lambda_image,lambda_speech)\n","        # assert n_elbo_terms == (len(attrs) + 1) + 1 + args.approx_m  # N + 1 + M\n","        train_loss_meter.update(train_loss.data, len(image1))\n","        \n","        # compute and take gradient step\n","        train_loss.float().backward()\n","        optimizer.step()\n","\n","        \n","\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAnnealing-Factor: {:.3f}'.format(\n","                epoch, batch_idx * batch_size, N_mini_batches*batch_size,\n","                100. * batch_idx / N_mini_batches, train_loss_meter.avg, annealing_factor))\n","    \n","    \n","    \n","    History=mean_history(history,History)\n","\n","\n","\n","    print('====> Epoch: {}\\tLoss: {:.4f}'.format(epoch, train_loss_meter.avg))\n","    return History\n","def val(epoch,History):\n","    model.eval()\n","    history=init_history()\n","    for batch_idx, ((image1,image2,speech),y) in enumerate(val_loader):\n","        # count+=1\n","        if epoch < annealing_epochs:\n","            # compute the KL annealing factor for the current mini-batch in the current epoch\n","            annealing_factor = (float(batch_idx + (epoch - 1) * N_mini_batches_val + 1) /\n","                                float(annealing_epochs * N_mini_batches_val))\n","        else:\n","            # by default the KL annealing factor is unity\n","            annealing_factor = 1.0\n","        if CUDA:\n","            image1     = image1.cuda()\n","            image2     = image2.cuda()\n","            speech     = speech.cuda()\n","            y     = y.cuda()\n","        image1         = Variable(image1)\n","        image2         = Variable(image2)\n","        speech         = Variable(speech)\n","        y         = Variable(y)\n","        batch_size = image1.size(0)\n","        \n","        history,valid_loss= tabulate(model,history,image1,image2,speech,y,batch_idx,annealing_factor,lambda_image,lambda_speech)\n","        # val_loss+=history['m1m2m3']['elbo']\n","        \n","        # pbar.update()\n","    History=mean_history(history,History)\n","    # pbar.close()\n","    val_loss=History['m1m2m3']['elbo'][-1]\n","    print('====> Test Loss: {:.4f}'.format(val_loss))\n","    return val_loss,History"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Igg6T_qbFAPP"},"source":["def val1(epoch):\n","    model.eval()\n","    val_loss = 0\n","    count=0\n","    # for simplicitly, here i'm only going to track the joint loss. \n","    # pbar = tqdm(total=len(test_loader))\n","    for batch_idx, ((image1,image2,speech),y) in enumerate(val_loader):\n","        count+=1\n","        if epoch < annealing_epochs:\n","            # compute the KL annealing factor for the current mini-batch in the current epoch\n","            annealing_factor = (float(batch_idx + (epoch - 1) * N_mini_batches_val + 1) /\n","                                float(annealing_epochs * N_mini_batches_val))\n","        else:\n","            # by default the KL annealing factor is unity\n","            annealing_factor = 1.0\n","        if CUDA:\n","            image1     = image1.cuda().float()\n","            image2     = image2.cuda().float()\n","            speech     = speech.cuda().float()\n","            y     = y.cuda().float()\n","        image1         = Variable(image1)\n","        image2         = Variable(image2)\n","        speech         = Variable(speech)\n","        y         = Variable(y)\n","        batch_size = image1.size(0)\n","        \n","        recon_image1, recon_image2, recon_speech, recon_label, mu, logvar = model(image1,image2,speech)\n","        elbo,image1_bce,image2_bce,speech_mse,label_ce= elbo_loss(recon_image1,image1,recon_image2, image2,recon_speech, speech,recon_label,y , mu, logvar, \n","                            lambda_image=lambda_image, lambda_speech=lambda_speech,\n","                            annealing_factor=annealing_factor)\n","        val_loss+=elbo\n","        # pbar.update()\n","\n","    # pbar.close()\n","    val_loss /= count\n","    print('====> Test Loss: {:.4f}'.format(val_loss))\n","    return val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjAQrZrPFHuu"},"source":["best_loss = 100000\n","best_val_loss=1000000\n","count=0\n","History=init_history()\n","# History_val=init_history()\n","best_epoch=0\n","val_losses=[]\n","for epoch in range(1, EPOCHS + 1):\n","    History=train(epoch,History)\n","    print(History)\n","    # val_loss,History_val=val(epoch,History_val)\n","    # loss = test(epoch)\n","    val_loss=val1(epoch)\n","    # val_losses.append(val_loss)\n","    is_best = val_loss < best_loss\n","    if is_best:\n","        best_epoch=epoch\n","    if is_best:\n","        save_checkpoint({\n","        'state_dict': model.state_dict(),\n","        'best_loss': best_loss,\n","        'n_latents': N_LATENTS,\n","        'optimizer' : optimizer.state_dict(),\n","    }, is_best, folder=OUT_DIR)\n","    if (val_loss>best_loss) or torch.abs(val_loss-best_loss)<1e-1:\n","        # print(val_loss,best_loss)\n","        count+=1\n","        if count==4:\n","            History[\"best_epoch\"]=best_epoch\n","            History[\"best_loss\"]=best_loss\n","            History[\"val_loss\"]=val_losses\n","            with open(OUT_DIR+'/train_history.pkl','wb') as fout:\n","                pkl.dump(History, fout, protocol=pkl.HIGHEST_PROTOCOL)\n","            # with open(OUT_DIR+'/val_history.json','rb') as f:\n","            #     json.dumps(History_val,f)\n","            break\n","    else:\n","        count=0\n","    best_loss = min(val_loss, best_loss)\n","    # save the best model and current model\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjKU-rRkY6q4"},"source":["with open(OUT_DIR+'/train_history.pkl','rb') as f:\r\n","    History=pkl.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwkyCaEei6l1"},"source":["for keys in History.keys():\n","    print(best_epoch)\n","    print(keys)\n","    print(History[keys]['label_acc'][best_epoch-1])\n","    # print(History[keys]['image2_bce'][0])\n","# print(History['total_loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8IUH3v6XKiD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M193Hb2-pL0E"},"source":["print(History['m1'].keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d-bfWSuRXVP5"},"source":["#added manually due to cuda issues\r\n","val_losses=[359.17,282.2,256.1,241.4,229.1,222.5,215.88,211.67,209,206.6,204.1,202,201.2,199.75,198.72,198.68,198.27,197.4, 196.4,195.34,196.36,195.1,194.58,194.93,193.89,193.87,193.98,193.51,194.35,193.75,193.74,193.42]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_9dr-E6jfBh"},"source":["import matplotlib.pyplot as plt\n","x=np.arange(len(val_losses))\n","loss=History['m1m2m3']['elbo'][:]\n","acc=History['m3']['label_acc'][:]\n","fig, ax1 = plt.subplots()\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('ELBO')\n","# ax1.plot(t, data1, color=color)\n","ax1.tick_params(axis='y')\n","ax1.plot(x,loss)\n","ax1.plot(x,val_losses)\n","# plt.plot(x,acc)\n","# plt.xlabel('Epochs')\n","# plt.ylabel('ELBO')\n","plt.title('Figure 4: Loss(ELBO) and label accuracy convergence')\n","ax1.legend(['Training Loss','Validation Loss'])\n","# \n","\n","ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n","\n","color = 'tab:red'\n","ax2.set_ylabel('Accuracy')  # we already handled the x-label with ax1\n","ax2.plot(x, acc, color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","ax2.legend(['Accuracy'],loc='center right')\n","fig.tight_layout()\n","# plt.annotate('learning rate=1e-3', xy=(10, 1800), xytext=(15, 1600))\n","plt.savefig(OUT_DIR+'/final_loss.png')\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRzhaGQllls3"},"source":["print(len(History['m1m2m3']['label_acc'][:]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fRpCIJXlmI6"},"source":["model=load_checkpoint(OUT_DIR+'/checkpoint.pth.tar',use_cuda=True)\r\n","device = torch.device(\"cuda\")\r\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DjRg-eNLYeD"},"source":["model.eval()\r\n","val_loss = 0\r\n","count=0\r\n","# for simplicitly, here i'm only going to track the joint loss. \r\n","# pbar = tqdm(total=len(test_loader))\r\n","for _ in range(10):\r\n","    for batch_idx, ((image1,image2,speech),y) in enumerate(val_loader):\r\n","        imorig1=image1\r\n","        imorig2=image2\r\n","        # print(torch.argmax(y))\r\n","        label=np.argmax(y).cpu().detach().numpy()\r\n","        \r\n","        if label==1:\r\n","            print(label)\r\n","            break\r\n","    \r\n","    count+=1\r\n","    if CUDA:\r\n","        image1     = image1.cuda().float()\r\n","        image2     = image2.cuda().float()\r\n","        speech     = speech.cuda().float()\r\n","        y     = y.cuda().float()\r\n","    image1         = Variable(image1)\r\n","    image2         = Variable(image2)\r\n","    speech         = Variable(speech)\r\n","    y         = Variable(y)\r\n","    batch_size = image1.size(0)\r\n","\r\n","    recon_image1, recon_image2, recon_speech, recon_label, mu, logvar = model(speech=speech)\r\n","    # print(recon_image1.size())\r\n","    recon_image1=recon_image1.view(28,28)\r\n","    recon_image2=recon_image2.view(28,28)\r\n","    imorig1=imorig1.view(28,28)\r\n","    imorig2=imorig2.view(28,28)\r\n","    im1=recon_image1.cpu().detach().numpy()\r\n","    im2=recon_image2.cpu().detach().numpy()\r\n","    imorig1=imorig1.cpu().detach().numpy()\r\n","    imorig2=imorig2.cpu().detach().numpy()\r\n","    # if not os.path.isdir(OUT_DIR+\"/val_m2/\"):\r\n","    #     os.makedirs(OUT_DIR+\"/val_m2/\")\r\n","    plt.imshow(im1)\r\n","    plt.savefig(OUT_DIR+\"/val_m3/im1recon_\"+str(label)+\".png\")\r\n","    plt.show()\r\n","    plt.imshow(imorig1)\r\n","    plt.savefig(OUT_DIR+\"/val_m3/im1orig_\"+str(label)+\".png\")\r\n","    plt.show()\r\n","    plt.imshow(im2)\r\n","    plt.savefig(OUT_DIR+\"/val_m3/im2recon_\"+str(label)+\".png\")\r\n","    plt.show()\r\n","    plt.imshow(imorig2)\r\n","    plt.savefig(OUT_DIR+\"/val_m3/im2orig_\"+str(label)+\".png\")\r\n","    plt.show()\r\n","    if label==1:\r\n","        print(label)\r\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTltx_9ErCGd"},"source":["z= model.reparametrize(mu,logvar)\r\n","print(z.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oviV3lWh1C6C"},"source":["from copy import deepcopy\r\n","def discover_disentanglement(idx, change, num=100):\r\n","  z= model.reparametrize(mu,logvar).clone().detach()\r\n","  Z = torch.randn(num,512).to(device)\r\n","  sub = [-change*i for i in range(1,num//2+1)][::-1]\r\n","  add = [change*i for i in range(1,num//2+1)]\r\n","  print(sub+add)\r\n","  for i, ch in enumerate(sub+add):\r\n","    Z[i] = deepcopy(z)\r\n","    Z[i][idx] += ch \r\n","  return Z\r\n","tens=discover_disentanglement(25,5)\r\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcueIogW2ov9"},"source":["tens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOOA5hLF2qDf"},"source":["image1_recons=model.image1_decoder(tens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwY3xrxz5VMr"},"source":["image1_recons=image1_recons.view(-1,28,28)\r\n","image1_recons.size()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsxcThK-5ZY0"},"source":["import torchvision\r\n","from PIL import Image\r\n","im = image1_recons[0,:,:].cpu().detach().numpy()\r\n","plt.imshow(im)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQMrd_wq5-gu"},"source":["w=10\r\n","h=10\r\n","fig=plt.figure(figsize=(10, 10))\r\n","columns = 10\r\n","rows = 10\r\n","for i in range(0, columns*rows):\r\n","    img = image1_recons[i,:,:].cpu().detach().numpy()\r\n","    fig.add_subplot(rows, columns, i+1)\r\n","    plt.axis('off')\r\n","    # fig.set_aspect('equal')\r\n","    plt.subplots_adjust(wspace=0, hspace=0)\r\n","    plt.imshow(img,interpolation='nearest')\r\n","# plt.show()\r\n","plt.savefig(OUT_DIR+\"/temp.png\",bbox_inches='tight', pad_inches = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOF5Mfxu9EP3"},"source":[""],"execution_count":null,"outputs":[]}]}